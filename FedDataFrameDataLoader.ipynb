{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "182ce5dd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Loading CSV data in SecretFlow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8d6c44d1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The following codes are demos only. It's **NOT for production** due to system security concerns, please **DO NOT** use it directly in production."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "01776225",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This tutorial will demonstrate, through several examples, how to load CSV data in SecretFlow and utilize it for data processing and modeling."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e7323a0a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2842348",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68274f5f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The version of SecretFlow: 1.4.0.dev20240105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-10 03:56:13,551\tINFO worker.py:1538 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "import secretflow as sf\n",
    "\n",
    "# Check the version of your SecretFlow\n",
    "print('The version of SecretFlow: {}'.format(sf.__version__))\n",
    "\n",
    "# In case you have a running secretflow runtime already.\n",
    "sf.shutdown()\n",
    "sf.init(['alice', 'bob', 'charlie'], address=\"local\", log_to_driver=True)\n",
    "alice, bob, charlie = sf.PYU('alice'), sf.PYU('bob'), sf.PYU('charlie')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8af24c46",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Introduction to the interface"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ffd77211",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In SecretFlow, we provide an interface similar to `pandas.read_csv` to read CSV data from different parties and unify it into a federated concept of data.\n",
    "\n",
    "- For horizontal scenarios, there is `secretflow.horizontal.read_csv` [API](https://www.secretflow.org.cn/docs/secretflow/en/source/secretflow.data.horizontal.html#secretflow.data.horizontal.read_csv) available.\n",
    "- For horizontal scenarios, there is `secretflow.vertical.read_csv` [API](https://www.secretflow.org.cn/docs/secretflow/en/source/secretflow.data.horizontal.html#secretflow.data.vertical.read_csv) available. \n",
    "\n",
    "By using `read_csv`, you can read CSV files from multiple parties and create a FedDataFrame."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "531f5302",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Build Federated Table**  \n",
    "A federated table is a virtual concept that spans multiple parties.\n",
    "\n",
    "1. Data from each party in the federated table is stored locally and is not allowed to leave the domain.\n",
    "2. Except for the party that owns the data, no one else can access the data storage.\n",
    "3. Any operation on the federated table is scheduled by the Driver to each Worker, and the execution instructions are passed layer by layer until they reach the Python Runtime of the specific Worker that owns the data. The framework ensures that data can only be operated on when the Worker.device and Object.device are the same.\n",
    "4. The federated table is designed to manage and manipulate multi-party data from a central perspective.\n",
    "5. The interface aligns with `pandas.DataFrame` to reduce the cost of multi-party data operations.\n",
    "\n",
    "<img alt=\"vdataframe.png\" src=\"resources/vdataframe.png\" width=\"600\">  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b6b2af40",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data Download and Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed65fa48",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%%!\n",
    "wget https://secretflow-data.oss-accelerate.aliyuncs.com/datasets/iris/iris.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdf1afd6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "alldata_df = pd.read_csv(\"./iris.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88df4de9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "150"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(alldata_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ff06f1b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "h_alice_df = alldata_df.loc[:70]\n",
    "h_bob_df = alldata_df.loc[70:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6bd4a699",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Save horizontally split DataFrame as CSV files separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66d07e39",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# save the data to local file system\n",
    "import tempfile\n",
    "\n",
    "_, h_alice_path = tempfile.mkstemp()\n",
    "_, h_bob_path = tempfile.mkstemp()\n",
    "h_alice_df.to_csv(h_alice_path, index=False)\n",
    "h_bob_df.to_csv(h_bob_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "609b6f57",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "v_alice_df = alldata_df.loc[:, ['sepal_length', 'sepal_width']]\n",
    "v_bob_df = alldata_df.loc[:, ['petal_length', 'petal_width', 'class']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "43f0863d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Save vertically split DataFrame as CSV files separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c51826c9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# save the data to local file system\n",
    "_, v_alice_path = tempfile.mkstemp()\n",
    "_, v_bob_path = tempfile.mkstemp()\n",
    "v_alice_df.to_csv(v_alice_path, index=True, index_label=\"id\")\n",
    "v_bob_df.to_csv(v_bob_path, index=True, index_label=\"id\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "24b6916a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Loading CSV Data Example for Horizontal Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "713e09fe",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from secretflow.data.horizontal import read_csv\n",
    "from secretflow.security.aggregation.plain_aggregator import PlainAggregator\n",
    "from secretflow.security.compare.plain_comparator import PlainComparator\n",
    "from secretflow.data.split import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da57e9e3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "First, prepare the CSV data files for two parties. In a horizontal scenario, it is required that the schema of the data from both parties is consistent.\n",
    "\n",
    "- Alice: datapath (the local path accessible on Alice's machine)\n",
    "- Bob: datapath (the local path accessible on Bob's machine)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7a0d4ae1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In a horizontal scenario, since the data with the same schema is distributed across multiple parties, cross-domain computation is required for certain DataFrame operations.\n",
    "The `read_csv` interface requires the specification of an `aggregator` and a `comparator`. We can specify a `secure aggregator` and a `secure comparator` during computation to protect data privacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c4ffb66",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Create proxy actor <class 'secretflow.data.core.agent.PartitionAgent'> with party alice.\n",
      "INFO:root:Create proxy actor <class 'secretflow.data.core.agent.PartitionAgent'> with party bob.\n"
     ]
    }
   ],
   "source": [
    "path_dict = {alice: h_alice_path, bob: h_bob_path}\n",
    "\n",
    "aggregator = PlainAggregator(charlie)\n",
    "comparator = PlainComparator(charlie)\n",
    "\n",
    "hdf = read_csv(filepath=path_dict, aggregator=aggregator, comparator=comparator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d36b91b5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdf.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e0c6a4ae",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The resulting hdf is a FedDataFrame, and we can now perform some data processing on it.\n",
    "For reference, [Data Preprocessing with FedDataFrame](https://www.secretflow.org.cn/docs/secretflow/en/tutorial/data_preprocessing_with_data_frame.html)ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bec80d9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "label = hdf[\"class\"]\n",
    "data = hdf.drop(columns=\"class\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3f89bc5c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The obtained `data` and `label` can be inputted into FLModel or SLModel for modeling purposes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dfc3eb3f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "SecretFlow provides `train_test_split` to split data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e065f08",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(\n",
    "    data, train_size=0.8, shuffle=True, random_state=1234\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16dadfc1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{PYURuntime(alice): (56, 4), PYURuntime(bob): (64, 4)} {PYURuntime(alice): (15, 4), PYURuntime(bob): (16, 4)}\n"
     ]
    }
   ],
   "source": [
    "print(train_data.partition_shape(), test_data.partition_shape())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "849e2e9d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Loading CSV Data Example for Vertical Scenario"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "52ea476a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "First, prepare the CSV data files for two parties. In a vertical scenario, it is not required that the schema of the data from both parties is consistent.\n",
    "We provide the ability of PSI in the `read_csv` interface.\n",
    "\n",
    "- Alice: datapath (The local path accessible on Alice's machine)\n",
    "- Bob: datapath (The local path accessible on Bob's machine)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "75308a31",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In a vertical scenario, the schema of the data from both parties is not consistent, but each party has all the data of each column. No comparator or aggregator is required. However, the data of each party is not necessarily aligned, and we need to align the data through `PSI` when reading.\n",
    "\n",
    "\n",
    "- path_dict: data path\n",
    "- spu: spu device used for intersection\n",
    "- keys: keys for intersection (support multi-column intersection)\n",
    "- drop_keys: ID column name to be deleted after intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0665453",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spu = sf.SPU(sf.utils.testing.cluster_def(['alice', 'bob']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a7e0a93",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<secretflow.device.device.spu.SPU at 0x7fb738294880>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b30b5e0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from secretflow.data.vertical import read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aca648d6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Create proxy actor <class 'secretflow.data.core.agent.PartitionAgent'> with party alice.\n",
      "INFO:root:Create proxy actor <class 'secretflow.data.core.agent.PartitionAgent'> with party bob.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(SPURuntime(device_id=None, party=bob) pid=2557)\u001B[0m [2024-01-10 04:08:58.055] [info] [bucket_psi.cc:285] bucket size set to 1048576\n",
      "\u001B[2m\u001B[36m(SPURuntime(device_id=None, party=bob) pid=2557)\u001B[0m [2024-01-10 04:08:58.055] [info] [bucket_psi.cc:131] Begin sanity check for input file: /tmp/tmpl_ygsk_u, precheck_switch:true\n",
      "\u001B[2m\u001B[36m(SPURuntime(device_id=None, party=bob) pid=2557)\u001B[0m [2024-01-10 04:08:58.059] [info] [csv_checker.cc:132] Executing duplicated scripts: LC_ALL=C sort --buffer-size=1G --temporary-directory=/tmp --stable selected-keys.dec2a1d4-c012-45f2-bdac-3a8bd76d89dc | LC_ALL=C uniq -d > duplicate-keys.dec2a1d4-c012-45f2-bdac-3a8bd76d89dc\n",
      "\u001B[2m\u001B[36m(SPURuntime(device_id=None, party=bob) pid=2557)\u001B[0m [2024-01-10 04:08:58.064] [info] [bucket_psi.cc:146] End sanity check for input file: /tmp/tmpl_ygsk_u, size=150\n",
      "\u001B[2m\u001B[36m(SPURuntime(device_id=None, party=bob) pid=2557)\u001B[0m [2024-01-10 04:08:58.064] [info] [bucket_psi.cc:208] Skip doing psi, because dataset has been aligned!\n",
      "\u001B[2m\u001B[36m(SPURuntime(device_id=None, party=bob) pid=2557)\u001B[0m [2024-01-10 04:08:58.064] [info] [bucket_psi.cc:267] Begin post filtering, indices.size=150, should_sort=true\n",
      "\u001B[2m\u001B[36m(SPURuntime(device_id=None, party=bob) pid=2557)\u001B[0m [2024-01-10 04:08:58.064] [info] [bucket_psi.cc:275] End post filtering, in=/tmp/tmpl_ygsk_u, out=/tmp/tmpl_ygsk_u.psi_output_28994\n",
      "\u001B[2m\u001B[36m(SPURuntime(device_id=None, party=alice) pid=2556)\u001B[0m [2024-01-10 04:08:58.055] [info] [bucket_psi.cc:285] bucket size set to 1048576\n",
      "\u001B[2m\u001B[36m(SPURuntime(device_id=None, party=alice) pid=2556)\u001B[0m [2024-01-10 04:08:58.055] [info] [bucket_psi.cc:131] Begin sanity check for input file: /tmp/tmpzudrgjv0, precheck_switch:true\n",
      "\u001B[2m\u001B[36m(SPURuntime(device_id=None, party=alice) pid=2556)\u001B[0m [2024-01-10 04:08:58.059] [info] [csv_checker.cc:132] Executing duplicated scripts: LC_ALL=C sort --buffer-size=1G --temporary-directory=/tmp --stable selected-keys.d061bc10-e571-4aae-bd81-a5f1d9f1f7cf | LC_ALL=C uniq -d > duplicate-keys.d061bc10-e571-4aae-bd81-a5f1d9f1f7cf\n",
      "\u001B[2m\u001B[36m(SPURuntime(device_id=None, party=alice) pid=2556)\u001B[0m [2024-01-10 04:08:58.063] [info] [bucket_psi.cc:146] End sanity check for input file: /tmp/tmpzudrgjv0, size=150\n",
      "\u001B[2m\u001B[36m(SPURuntime(device_id=None, party=alice) pid=2556)\u001B[0m [2024-01-10 04:08:58.065] [info] [bucket_psi.cc:208] Skip doing psi, because dataset has been aligned!\n",
      "\u001B[2m\u001B[36m(SPURuntime(device_id=None, party=alice) pid=2556)\u001B[0m [2024-01-10 04:08:58.065] [info] [bucket_psi.cc:267] Begin post filtering, indices.size=150, should_sort=true\n",
      "\u001B[2m\u001B[36m(SPURuntime(device_id=None, party=alice) pid=2556)\u001B[0m [2024-01-10 04:08:58.065] [info] [bucket_psi.cc:275] End post filtering, in=/tmp/tmpzudrgjv0, out=/tmp/tmpzudrgjv0.psi_output_28994\n"
     ]
    }
   ],
   "source": [
    "path_dict = {\n",
    "    alice: v_alice_path,  # The path that alice can access\n",
    "    bob: v_bob_path,  # The path that bob can access\n",
    "}\n",
    "\n",
    "# Prepare the SPU device\n",
    "spu = sf.SPU(sf.utils.testing.cluster_def(['alice', 'bob']))\n",
    "\n",
    "vdf = read_csv(path_dict, spu=spu, keys='id', drop_keys=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d01c4d7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7640532b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "label = vdf[\"class\"]\n",
    "data = vdf.drop(columns=\"class\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc9d4ee4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can also use `train_test_split` to split data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "69a79fbb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(\n",
    "    data, train_size=0.8, shuffle=True, random_state=1234\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6640aa76",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Next, you can try your own csv data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "default",
   "language": "python",
   "display_name": "default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}